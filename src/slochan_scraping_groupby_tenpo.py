import bs4
import time
import pandas as pd
from PIL import Image,ImageFont,ImageDraw
import os
import requests  
import bs4
import time
import pandas as pd
import datetime
import urllib.parse
import datetime
from dotenv import load_dotenv
load_dotenv(".env")

def post_line_image_and_text(message,image_path,line_token):
    url = "https://notify-api.line.me/api/notify"
    headers = {"Authorization" : "Bearer "+ line_token}
    payload = {"message" :  message}
    #imagesãƒ•ã‚©ãƒ«ãƒ€ã®ä¸­ã®gazo.jpg
    print('image_path',image_path)
    files = {"imageFile":open(image_path,'rb')}
    post = requests.post(url ,headers = headers ,params=payload,files=files)

def post_line_text(message,line_token):
    url = "https://notify-api.line.me/api/notify"
    headers = {"Authorization" : "Bearer "+ line_token}
    payload = {"message" :  message}
    post = requests.post(url ,headers = headers ,params=payload)

def scraping_yesterday_groupby_prefecture_tenpo_data(prefecture:str) -> str:
    yesterday = datetime.date.today() - datetime.timedelta(days=1)
    date = yesterday.strftime('%Y-%m-%d')
    print(date)
    quote_prefecture = urllib.parse.quote(prefecture)
    url = f'https://ana-slo.com/{date}-{quote_prefecture}-hallpickup/'
    #print(url)    res = requests.get(url_1)
    res = requests.get(url)
    soup = bs4.BeautifulSoup(res.text, 'html.parser')
    dfs = pd.read_html(res.text)
    concat_df = pd.DataFrame(columns=[],index=[])
    main_content = soup.find(True, class_='entry-content')
    div_list = main_content.find_all(True, class_='')
    for i,div in enumerate(div_list):
        #print(i,str(div))
        if '<div>' in str(div):
            try:
                #print(div.find('a').text)
                concat_df
                tenpo_name = div.find('a').text
                _df = pd.read_html(str(div))[0]
                _df['åº—èˆ—å'] = tenpo_name
                concat_df = pd.concat([concat_df,_df],axis=0) 
            except:
                pass
    emoji_list = ['ğŸ¥‡','ğŸ¥ˆ','ğŸ¥‰','ğŸ–','ğŸ–']
    #æ–‡å­—åˆ— æ˜¨æ—¥ã‚’æ›œæ—¥ã‚‚åŠ ãˆã¦strå‹ã«å¤‰æ›
    week = ['æœˆ','ç«','æ°´','æœ¨','é‡‘','åœŸ','æ—¥']
    date = yesterday.strftime('%m').lstrip('0') + 'æœˆ' + yesterday.strftime('%d').lstrip('0') + 'æ—¥' \
    + '(' + week[yesterday.weekday()] + ')'
    #print(date)

    data_text =''
    for emoji ,(i,row) in zip(emoji_list,concat_df.iterrows()):
        #print(i,row)
        tenpo_name = row['åº—èˆ—å']
        sum_medal = '{:,}'.format(round(int(row['ç·å·®æš']), -2))
        if int(row['ç·å·®æš']) > 0:
            sum_medal = '+' + sum_medal + 'æš'

        ave_medal = str(row['å¹³å‡å·®æš']) 
        if int(row['å¹³å‡å·®æš']) > 0:
            ave_medal = '+' + ave_medal + 'æš'
        daisuu = row['å‹ç‡'].split('(')[1].replace(')','')+ 'å°'
        data_text += f'{emoji}{tenpo_name}\n ç·å·®{sum_medal} å¹³å‡{ave_medal} {daisuu}\n'
    #print(data_text)

    output_text = f'''\n\n\nã€ˆ{date}é€Ÿå ±ã€‰\nã€{prefecture.replace('éƒ½','').replace('çœŒ','')} å¹³å‡å·®æšãƒ©ãƒ³ã‚­ãƒ³ã‚°TOP5ã€‘\n\n'''
    #print(output_text)
    output_text += data_text
    #print(output_text)
    return output_text



import time         # ã‚¿ã‚¤ãƒãƒ¼ç”¨
import traceback    # ä¾‹å¤–æ¤œçŸ¥ç”¨
for _ in range(5):  # æœ€å¤§3å›å®Ÿè¡Œ
    try:
        for prefecture in ['ç¥å¥ˆå·çœŒ','åŸ¼ç‰çœŒ','åƒè‘‰çœŒ','æ±äº¬éƒ½']:
            output_text = scraping_yesterday_groupby_prefecture_tenpo_data(prefecture)
            post_line_text(output_text,os.getenv('SLOCHAN_LINE_TOKEN'))
            time.sleep(3)

    except Exception as e:
        post_line_text(f"{e} å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ç¹°ã‚Šè¿”ã—ã¾ã™",os.getenv('SLOCHAN_LINE_TOKEN'))
        print(traceback.format_exc()) # ä¾‹å¤–ã®å†…å®¹ã‚’è¡¨ç¤º
        time.sleep(3600) # é©å½“ã«å¾…ã¤
    else:
        print("æˆåŠŸã—ã¾ã—ãŸã€‚ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        break
else:
    post_line_text("æœ€å¤§è©¦è¡Œå›æ•°ã«é”ã—ã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™",os.getenv('SLOCHAN_LINE_TOKEN'))
    